# Practical Concepts {#practical_concepts} 

## Gradient Descent

Gradient descent is an algorithm that aims to minimize a function by finding a local minimum closest to the initialization value. The goal is to find some value $A$ such that $F(A)$ is the smallest possible. 
Let $F(x)$ be a function differentiable at initial value $A$. Then for each iteration of gradient descent, one updates the value of $A$ by “stepping” in the opposite direction of the gradient function.

$$A_{n+1} = A_{n} - \alpha \nabla F(A_n)$$

$\alpha$ is called the learning rate.  This gradient descent step is repeated for a number of iterations, which can be predetermined in advance, or the learning could be halted after meeting a certain condition.

A helpful visualization: suppose the function $F$ is the altitude of a mountain range, and the value $A$ is the location of a marble. Thus we minimize $F$ by finding the location on the mountain range with the lowest altitude. Using gradient descent, we drop the marble somewhere based on our initial guess. Due to gravity, the marble will roll towards the lowest area in its proximity.

A potential pitfall is that this algorithm finds a local minimum, but not the global minimum. See below for an example with initialization at $A$: the value we end at using gradient descent would be $B$. This may be problematic, given that the true minimum is at $C$.

![](img/gradient_descent.png)

Useful Resources:

- [Linear Regression using Gradient Descent (September 2018, Towards Data Science)](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)
- [Gradient Descent (Wikipedia)](https://en.wikipedia.org/wiki/Gradient_descent)

## Backpropagation

Useful Resources:

- [CS 231N: Understanding Backpropagation](http://cs231n.github.io/optimization-2/)

## Cross-Validation & Hyperparameter Tuning

A typical machine learning model relies on two categories of inputs, or "parameters": model parameters - the weights learned from the data - and learning parameters or hyperparameters - general configuration settings for the model. Determining the best hyperparameters for the task is a crucial part of machine learning.  

Model parameters are learned from the data. They typically reflect how heavily to weight input features in a prediction. For example, if an algorithm determines square footage to be twice as important as number of bathrooms in determining a house’s value, then the weight corresponding to square footage would be double that corresponding to number of bathrooms. When the model learns a line `y = Wx + b` in linear regression, `W` and `b` are model parameters. These parameters are learned from the data and are required to make predictions.  

However, training a model usually requires setting several other "hyperparameters"" that define how the model is structured and trained. These include the learning rate (how fast to replace old estimations with new ones), along with parameters necessary to configure different models, like the number of layers in a neural network, or the value of k in k-nearest-neighbors. The process of deciding which hyperparameters result in the best model performance is called “hyperparameter tuning.” This is a time-consuming but important process for most machine learning tasks. 

Learning hyperparameters requires a separate "validation set", distinct from the training and test sets. Using a separate dataset is crucial; using the test set to test hyperparameter performance would be poor practice. One of the sacred rules of machine learning is to never use the test data to set parameters. Training on the test set risks overfitting the model to the test set, causing it to fail to generalize to new data in the real world. Instead, all parameters should be estimated before the model ever sees the test set, so that it can be fairly evaluated on unseen data. 

This technique is called cross-validation. For example, you might set aside 20% of the training data as a validation set. After estimating model parameters from the training set, different hyperparameters can be tested on the validation set. An even more reliable way to evaluate the performance of a set of hyperparameters is to create k splits of the data. Iterate over the k splits; on each iteration, select that split to be the validation set and train on the rest of the splits combined. To evaluate the performance of a set of hyperparameters, average its performance over all k iterations. This is known as k-fold cross-validation. K-fold cross-validation is less susceptible to overfitting, but is more computationally expensive.

Useful Resources:

- [CS 231N: Validation Sets for Hyperparameter Tuning](http://cs231n.github.io/classification/#val)

## Transfer Learning

Transfer learning, as its name suggests, is the technique of using knowledge learned from one task and applying it to a new but related task. For example, knowledge learned while trying to figure out how to classify images (of animals) into various animal groups can be transferred to some degree to the task of classifying images (of dogs) into various dog breeds. 

The main motive behind transfer learning is to reduce training requirements on both dataset size and computational power. In practice, large architectures, especially CNNs, are rarely trained from scratch because doing so requires not only a sufficiently large dataset, but also, in turn, sufficient computational power. It is more common to take a pre-trained model on a related task and apply it to the problem at hand. 

Generally, there are three broad ways this application can look:

1. Direct application. Simply apply the pre-trained model directly to the new task. This is probably the least common of the three because it is not trained specifically for the new and presumably somewhat different task.
2. Feature extraction. Given a set of inputs, a pre-trained model (after removing the last output layer) can be used to vectorize each input into a list of features. The resulting input vectors can then be fed through a smaller classifier (e.g. any of the non-DL classifiers described above) to train a model on the new task. 
    - When to use: Dataset is small. The reason is that fine-tuning a pre-trained model on a 
small dataset can lead to overfitting. Furthermore, it is important to select the appropriate layer of the pre-trained model to use as feature extractor. For example, if the inputs of the new task are quite related to those of the pre-trained model, you can consider using the second-to-last layer (basically the entire architecture minus the final output layer). The more unrelated the inputs, the earlier the feature extractor layer chosen should be because earlier layers capture more generic features, while later layers capture more dataset-specific features.
3. Fine-tuning. In addition to replacing and training the classifier on top of the pre-trained model (as in the Feature Extraction application), this application also involves re-training, or fine-tuning, the weights of the pre-trained model. It is possible to choose which weights to fine-tune, and in fact, it is common to keep the weights of earlier layers fixed, and only fine-tune the later weights depending on how related the datasets are. It is important to keep the learning rate small when fine-tuning to avoid excessively perturbing the pre-trained weights, as we expect them to already be fairly good.
    - When to use: Dataset is large. When the dataset is large, it is less likely that fine-tuning will lead to overfitting, and as a result, we can confidently re-train the model weights. A sufficiently large dataset also allows for more effective training, especially for a deep neural net. As alluded to above, the more unrelated the datasets, the more weights we should re-train (keeping fewer weights fixed). Even if the datasets are very unrelated, in practice, it is often still advantageous to initialize with pre-trained weights and just fine-tune all of them. 

- [CS 231N: Transfer Learning](http://cs231n.github.io/transfer-learning/)
- [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning (November 2018, Towards Data Science)](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)

## Data Augmentation


## AutoML

Currently, a big limiting factor in scaling up the application of machine learning to various domains is the need for a team of specialists to support the end to end ML process. Everything from the ingestion of data, to data pre-processing, to feature engineering and model interpretation, requires individuals who deeply understand the various facets of the pipeline. 

![](img/automl.png)

Automated machine learning (autoML) seeks to abstract away much of the technical processes required for each of these stages. It helps to automate the end to end machine learning pipeline to enable more non-experts to develop and scale effective and robust predictive models.

There are some broad categories of autoML, namely, automated data preparation and ingestion, automated feature engineering and hyperparameter tuning as well as automated model selection. Different autoML providers target different stages of the pipeline. For instance, the open source packages auto-sklearn and ATM (auto-tune models) automate hyperparameter optimization and model selection, while TPOT creates entire end to end pipelines using genetic programming. 

**Limitations**

- While autoML is useful in abstracting away the nitty gritty details of ML implementation, it could potentially exacerbate model complexity. Without model interpretability, there might be a tendency to treat the autoML models as a black box. As a black box, model predictions might be difficult to understand and interpret. While open source autoML packages such as Featuretools has a built in module to explain predictions, many autoML packages still do not.
- Further, autoML typically works well in supervised learning scenarios where there is a clear measure of success. In unsupervised or reinforcement learning environments, there does not exist an objective function which automated models could optimize for easily.
- Finally, feature extraction and engineering is still best performed by specialists with embedded knowledge expertise. While autoML tools can infer generic feature types from similarly structured datasets, the best features still come from heuristics based on experience in the field.

**Current AutoML commercial providers**

- [Auger.ai](http://auger.ai)
- DataRobot
- Google AutoML
- Microsoft Azure AutoML
- OneClick.ai

**Current AutoML open source packages**

- Auto-sklearn
- Auto-Tune Models
- Caret
- H2O
- TPOT

**Resources**

- [Building AI to Build AI at NeurIPS (Kdnuggets, January 2019)](https://www.kdnuggets.com/2019/01/building-ai-to-build-ai-neurips-automl-challenge.html)
- [Why AutoML Won't Replace Data Scientists Yet (Kdnuggets, March 2019)](https://www.kdnuggets.com/2019/03/why-automl-wont-replace-data-scientists.html)
- [AutoML, the next wave of machine learning](https://heartbeat.fritz.ai/automl-the-next-wave-of-machine-learning-5494baac615f)